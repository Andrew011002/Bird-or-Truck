{"cells":[{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":7847,"status":"ok","timestamp":1648061010967,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"Jl7g_DpaFwdL"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# importanting necessary to solve image classification problem\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6559,"status":"ok","timestamp":1648061017515,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"BxHxWrpxFwdN","outputId":"6233fbb8-a2e0-404f-a8e1-5b19e7e218fb"},"outputs":[],"source":["\"\"\"\n","Now that we have all the necessary libaries for our problem, we\n","need to load in our data. The data we will be importing will\n","be 32 x 32 pixel colored images of vehicles and animals. The volume of \n","training data is 50,000 colored images and 10,000 testing images.\n","Let's load in the data and dive into what will be fed into the model\n","\"\"\"\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1648061018036,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"vCBpz-vVFwdO","outputId":"38718b75-c38b-448b-ce42-9178290cf370"},"outputs":[{"name":"stdout","output_type":"stream","text":["training shapes: x: (50000, 32, 32, 3), y: (50000, 1)\n","testing shapes: x: (10000, 32, 32, 3), y: (10000, 1)\n","[4]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\"\"\"\n","With the data sucessfully loaded, let's take a look \n","at the shape of the data as well as what it actually \n","looks like to a human. For verification, our shapes should\n","match up with our preivous claim above along with rgb values\n","on the x datasets and classifications for the y datasets. Our data input\n","data should also be e dimensions because of the RGB values of them.\n","\"\"\"\n","print(f\"training shapes: x: {x_train.shape}, y: {y_train.shape}\\ntesting shapes: x: {x_test.shape}, y: {y_test.shape}\")\n","\n","plt.imshow(x_train[0]) # looks like our first peice of data is a frog\n","print(y_train[0]) # looks like our first peice of data is a frog\n","\n","\n"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":16352,"status":"ok","timestamp":1648061034383,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"z9hMsNUHFwdP"},"outputs":[],"source":["\"\"\"\n","With the images read in let's consider how they \n","will be processed into a model. Since the images are not\n","that big in size (32x32 pixel), they do not need\n","to be re-sized, although the images are colored. One\n","thing I want to consider is gray scaling the images\n","to make processing less complex. In order to do this\n","I need to apply the weighted sum to each image.\n","\"\"\"\n","\n","def process(images):\n","    data = [] # new array tp hold gray scale images\n","    # images = images / 255.0 # normalizing every image between 0 and 1\n","    gamma = 1.04 # color gamma correction factor (smoother color transitions)\n","    r_cst, g_cst, b_cst = 0.2126, 0.7152, .0722 # constants for red, green, and blue gray scale conversion factor\n","    for img in images:\n","        r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2] # each rgb value for the image\n","        data.append((r_cst * r ** gamma) + (g_cst * g ** gamma) + (b_cst * b ** gamma))\n","    return np.array(data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_train, x_test = process(x_train), process(x_test)"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1648061034682,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"7WuMYERHFwdQ","outputId":"6aa139a0-1dd2-4423-caff-8fecf6149bac"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","\"\"\"\n","Looks like our images are now grey scaled with their\n","rgb values averaged based on their weighted luminance.\n","Here is a quick representation of what our new data looks\n","like as well as the values with in the image array.\n","With the elimination of our rgb values, our images are no\n","longer 2 dimensions but 3. We can see this with the new shapes\n","\"\"\"\n","\n","\n","f = plt.figure()\n","f.add_subplot(1,2, 1)\n","plt.imshow(x_train[0], cmap=plt.cm.get_cmap(\"gray\"))\n","f.add_subplot(1,2, 2)\n","plt.imshow(x_test[0], cmap=plt.cm.get_cmap(\"gray\"))\n","plt.show(block=True)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1648061034847,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"FY5M2kvxFwdR","outputId":"2e4b2571-2b58-45d0-de0f-f94018f2c135"},"outputs":[],"source":["\"\"\"\n","With the data now ready to go, let's build\n","a baseline model that can give us an idea on \n","how to solve our image processing problem. I find\n","Tensorflows keras library to be the most intuitive\n","and the most powerful of all the Machine Learning\n","libraries so I will use that libarary and it's \n","parameters to build a base. \n","\"\"\"\n","\n","from tensorflow.keras import Sequential # model to stack layers\n","from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, Dropout\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","\"\"\"\n","Since the problem is image classification, building a \n","Convolutional Neural Network is probably the best for this\n","problem. Keras has some classes from the layers class\n","that will help us create a CNN, so I will use those \n","as well as some other classes that will help us later on.\n","Finally, I will use Tensorboard to help me visual the \n","metrics of the model on a plot.\n","\n","Before we go into developing that, let's normalize the data\n","and prepare a baseline model to compare to our CNN models.\n","All we have to do is divide by the max range of of pixel values\n","to obtain a value between 0 and 1\n","\"\"\";\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_train, x_test = x_train / 255.0, x_test / 255.0\n","print(f\"new training point:\\n{x_train[0]}\\nnew test point:\\n{x_test[0]}\")"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":147,"status":"ok","timestamp":1648061034990,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"AaRgohd9FwdS"},"outputs":[],"source":["\"\"\"\n","Now that we have the data, we can begin building our baseline\n","model. Since the the images are fed forward sequentially, we will\n","be using the sequential model. First, I will initialize it then\n","add features that will allow it to be trainable\n","\"\"\"\n","\n","baseline = Sequential() # init"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169,"status":"ok","timestamp":1648061035154,"user":{"displayName":"Andrew Holmes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyT9MjOFQL2ICZPq0NOqdY2QEhPnmXbu96wJzTrQ=s64","userId":"13991702313640313284"},"user_tz":240},"id":"zrYZh99uFwdT","outputId":"4fbe80e9-701d-4ded-c9cd-85127dc876ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_67\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_66 (Flatten)        (None, 1024)              0         \n","                                                                 \n"," dense_102 (Dense)           (None, 128)               131200    \n","                                                                 \n"," dense_103 (Dense)           (None, 10)                1290      \n","                                                                 \n","=================================================================\n","Total params: 132,490\n","Trainable params: 132,490\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["\"\"\"\n","With the model created, let's start adding \n","functionality to it. Since the model is baseline\n","I will keep it simple to get us in the right direction\n","for building a real Convolutional Neural Network.\n","\n","Because the images are 2D, I will modify them so every pixel in the image represents one single\n","input neuron. The Flatten layer helps do this.\n","\n","Next I want a single hidden layer of neurons with an activation function that will help learn\n","the 1024 (32x32 flattened) inputs. I will add a Dense (fully connected layer) layer with 128 neurons\n","outputing a value from the inputs. For the activation, I will choose rectified linear because of it's\n","convergence advantages and speed\n","\n","Lastly, I will add our output layer which will fire a neuron based on the highest value between all neurons\n","in the output layer. Since we have 10 classes, I will be using an output layer of 10 neurons and an activation\n","of softmax which helps precisely determine the neuron of highest activity to fire.\n","\"\"\"\n","\n","baseline.add(Flatten(input_shape=(32, 32))) # flattening image from 2D -> 1D 1024 Input Layer\n","baseline.add(Dense(128, activation=\"relu\")) # 128 Hidden Layer relu\n","baseline.add(Dense(10, activation=\"softmax\")) # 10 Output Layer softmax\n","\n","baseline.summary() # network pipline\n","\n"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BufJATnmFwdU","outputId":"d0554cd0-f763-478a-fcd6-1167436422ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","1563/1563 [==============================] - 9s 5ms/step - loss: 2.0497 - accuracy: 0.2531\n","Epoch 2/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 1.9205 - accuracy: 0.3136\n","Epoch 3/10\n","1563/1563 [==============================] - 7s 4ms/step - loss: 1.8680 - accuracy: 0.3353\n","Epoch 4/10\n","1563/1563 [==============================] - 6s 4ms/step - loss: 1.8375 - accuracy: 0.3473\n","Epoch 5/10\n","1563/1563 [==============================] - 6s 4ms/step - loss: 1.8153 - accuracy: 0.3535\n","Epoch 6/10\n","1563/1563 [==============================] - 6s 4ms/step - loss: 1.7974 - accuracy: 0.3614\n","Epoch 7/10\n","1563/1563 [==============================] - 6s 4ms/step - loss: 1.7829 - accuracy: 0.3654\n","Epoch 8/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 1.7694 - accuracy: 0.3697\n","Epoch 9/10\n","1563/1563 [==============================] - 6s 4ms/step - loss: 1.7588 - accuracy: 0.3747\n","Epoch 10/10\n","1563/1563 [==============================] - 7s 4ms/step - loss: 1.7495 - accuracy: 0.3778\n","313/313 [==============================] - 1s 3ms/step - loss: 1.7805 - accuracy: 0.3644\n","Validation Loss: 1.7804551124572754\n","Validation Accuracy: 36.43999993801117%\n"]}],"source":["\"\"\"\n","With the model built let's begin to compile it towards a goal.\n","For the optimizer (how the model learns) I will use adam. I chose\n","adam because of it's efficiency and commonly with a large amount of\n","parameters. \n","\n","For the loss function, I will use Categorical Cross Entropy. This\n","computes the cross entropy from our prediction and the target of a given data point\n","and is good for multi-class error minimaztion. Since our labels are integer and not\n","one-hot, I will use the Sparse algorithm for the base model and all models because \n","our desired output remains as multi-classification.\n","\n","Lastly, I will choose the base accuracy for all models since all we want is the total\n","correct over the total data it has been given. \n","\n","After compilation, let's fit and evaluate the model using some model methods built-in.\n","I'm going to keep it minimal and go with 10 epochs which should be a valid amount of times\n","for the model to learn something but not try to fit to well to the training data.\n","\"\"\"\n","\n","baseline.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","baseline.fit(x_train, y_train, epochs=10)\n","loss, accuracy = baseline.evaluate(x_test, y_test)\n","print(f\"Validation Loss: {loss}\\nValidation Accuracy: {accuracy*100}%\")\n"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"O8d0vUmDFwdV"},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction: 4 True: 3\n"]}],"source":["# example prediction\n","print(f\"Prediction: {np.argmax(baseline.predict(x_test)[0])} True: {y_test[0][0]}\")\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"YjA_fHBnFwdV"},"outputs":[],"source":["\"\"\"\n","With the model evaluated, it is most certainly\n","not a good a idea to maintain these metrics for\n","real world use. The baseline model has an accuracy\n","in the 35-40 percent range and struggles to minimize\n","loss as it goes through epoch the epochs.\n","\n","Some things I'm considering for the two fine tuned models\n","are amount of hidden layers, neurons in them, \n","and potentially changing the learning rate of our optimizer or\n","the optimizer entirely to see if that helps learning.\n","\n","Something I learned when flattening the images is that I lose\n","the positional information of pixel. To eliminate this\n","I need to intro convolutional layers which will help\n","me maintain that information, extract features from them, and\n","hopefully help the model learn the image for classification. Then,\n","I should be able to flatten those features to our output layer\n","for classification.\n","\n","Lastly, something I didn't do previous was consider the batch size\n","when fitting. During the process of building the 2 models, I will\n","try different batch sizes to see which size has a better peformance\n","metric on our validation data.\n","\"\"\"\n","\n","pass\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"BqC-1bSiFwdW"},"outputs":[],"source":["\"\"\"\n","For our first tuned model lets build a \n","sequential model that adds convolutional layers\n","to some of the Hidden layers. \n","\n","One reason to include convolutional layers is to learn\n","features of the images. This is extremely important because\n","of the amount of inputs in a single image. In this example, there\n","are 1024 parameters which means as we add more hidden layers and\n","neurons in the hidden layers, the amount of trainiable weights grows\n","by a factor which can lead to over-fitting on a dataset and it's also \n","computationally intensive, especially for my laptop.\n","\n","One thing a convolutional layer does is provide a kernal from edge-to-edge\n","on an image. This kernal is basically the weights and a bias for an image peice. \n","Using convolutional operations between a space on an image and a kernal \n","of our image, we can obtain a single value. We can then apply some activation\n","function to this value. In total, this elimates the large amount of weights\n","and allows the extraction of features in an image to help us learn. For every kernal\n","we apply to the image, we create feature maps of the image. This is important because\n","we can apply what is known as max pooling which allows to obtain the most intense\n","values for inputs to next layers. Lastly, we can stack kernals to create filters which\n","allows us to have multiple feature maps. Since we're working with 10 classes of images\n","we will need filters.\n","\n","In combination of convolutional layers and pooling layers, we should see some peformance \n","increase in our model when we train and validate it using our datatsets.\n","\"\"\"\n","\n","pass"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"J4wdIGL3FwdX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_23\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_41 (Conv2D)          (None, 30, 30, 32)        320       \n","                                                                 \n"," max_pooling2d_41 (MaxPoolin  (None, 15, 15, 32)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_42 (Conv2D)          (None, 13, 13, 32)        9248      \n","                                                                 \n"," max_pooling2d_42 (MaxPoolin  (None, 6, 6, 32)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_23 (Flatten)        (None, 1152)              0         \n","                                                                 \n"," dense_43 (Dense)            (None, 10)                11530     \n","                                                                 \n","=================================================================\n","Total params: 21,098\n","Trainable params: 21,098\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["\"\"\"\n","For my first fine tuned model, I am getting the idea to\n","have two convolutional layers and two max pooling layers.\n","My ideology behind this is to create multiple feature maps\n","based on our filters then to pool the most important features \n","from those features maps. I then one to repeat the process \n","again on those pooled feature maps to extract further features \n","from the images. Lastly, I will then flatten the outputs of the last\n","pooled feature maps to give our output layer the ability to apply\n","the softmax activation on 1D data.\n","\n","The first convolutional layer will take our 32x32 images with 1 channel\n","and apply 32 3x3 kernals (filter) to the image. We will then apply the relu\n","activation function to those convolved values to create a cube like structure \n","of those values, 30x30x32 precisely. We will then downscale the 32 feature maps\n","by getting the highest intensity value over a 2x2 window. This will give us \n","a 32 15x15 pooled feature maps. I will repeat the process except the output\n","of the next convolutional layer will be a 13x13x32 cube and the pooled features\n","will be 32 6x6 pooled feature maps. \n","\n","Now that we extracted some important features, I will flatten those outputs\n","to make the information one dimensional. Once the outputs from the feature maps\n","are flattened we can then pass them to the softmax output layer to get a vector\n","of probabilities that indicate what class the input belongs to.\n","\n","Already looking at the summary of our baseline model to our first tuned model, our\n","trainable parameters is roughly 6x less which should really optimize training\n","\"\"\"\n","model_1 = Sequential()\n","\n","model_1.add(Conv2D(32, (3, 3), input_shape=(32, 32, 1), activation=\"relu\")) # Input -> Conv Layer 32 3x3 filter relu -> Feature Maps\n","model_1.add(MaxPooling2D(pool_size=(2, 2))) # Feature Maps -> Pooled Feature Map over 2x2 window\n","model_1.add(Conv2D(32, (3, 3), activation=\"relu\")) # Pooled Feature Maps -> Conv Layer 32 3x3 filter relu -> Feature Maps\n","model_1.add(MaxPooling2D(pool_size=(2, 2))) # Feature Maps -> Pooled Feature Map over 2x2 window \n","model_1.add(Flatten()) # Flattening Pooled Feature Map values\n","model_1.add(Dense(10, activation=\"softmax\")) # Output Layer softmax\n","model_1.summary()\n","\n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"FAtvREihFwdX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","1563/1563 [==============================] - 23s 14ms/step - loss: 1.6668 - accuracy: 0.4137\n","Epoch 2/10\n","1563/1563 [==============================] - 24s 15ms/step - loss: 1.3503 - accuracy: 0.5369\n","Epoch 3/10\n","1563/1563 [==============================] - 23s 14ms/step - loss: 1.2396 - accuracy: 0.5776\n","Epoch 4/10\n","1563/1563 [==============================] - 22s 14ms/step - loss: 1.1729 - accuracy: 0.6016\n","Epoch 5/10\n","1563/1563 [==============================] - 22s 14ms/step - loss: 1.1222 - accuracy: 0.6179\n","Epoch 6/10\n","1563/1563 [==============================] - 23s 14ms/step - loss: 1.0892 - accuracy: 0.6295\n","Epoch 7/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 1.0594 - accuracy: 0.6393\n","Epoch 8/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 1.0326 - accuracy: 0.6484\n","Epoch 9/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 1.0091 - accuracy: 0.6568\n","Epoch 10/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 0.9881 - accuracy: 0.6628\n","313/313 [==============================] - 1s 3ms/step - loss: 1.0903 - accuracy: 0.6269\n","Validation Loss: 1.0902934074401855\n","Validation Accuracy: 62.69000172615051%\n"]}],"source":["\"\"\"\n","With the model ready to go, lets compile it, then fit it to our\n","training data. I will use the same compilation parameters as before\n","for now to see how this model does.\n","\n","I will also maintain the same amount of epochs as for the previous\n","fit parameters of the base model\n","\"\"\"\n","\n","model_1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","model_1.fit(x_train, y_train, epochs=10)\n","loss, accuracy = model_1.evaluate(x_test, y_test)\n","print(f\"Validation Loss: {loss}\\nValidation Accuracy: {accuracy*100}%\")\n","\n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"iQdZBhWkFwdY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction: 1 True: 8\n"]}],"source":["print(f\"Prediction: {np.argmax(model_1.predict(x_test)[1])} True: {y_test[1][0]}\") # example prediction\n"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"TrcneLAzFwdZ"},"outputs":[],"source":["\"\"\"\n","With the model evaluated, we are moving in the right direction when it comes\n","to predicting the correct classification on a given image. It appears that\n","adding convolutional layers and pooling features from the feature maps created\n","from convolutional layers yields much better accuracy metrics than our baseline\n","model. The loss is noticably lower than the baseline model (lower means better) \n","and our accuracy has almost doubled from 35-40 percent range to the 60-70 percent\n","range.\n","\n","These are great leaps for our goal of maximizing our accuracy on the model. \n","With this first tuned model, I definitely want to take away some settings\n","for creating our final tuned model. \n","\n","I think the pairing of convolutional layers with max pooling layers to be a great\n","layout for the model. Although, I do want to consider changing some settings with in\n","the layers. Some things I want to consider is the amount of kernals I use on a given\n","filter. Perhaps trying 16, 64, or 128 etc. I also want to consider padding when\n","we create feature maps and pooled feature maps. Padding will give us the ability\n","to keep a feature map the same size as its input or allow us to enlarge it. Perhaps \n","doing this will help learning features of an image. \n","\n","Another aspect I want to touch on is adding more Dense (fully connected) layers\n","to the model. In my first fine tuned model, we only have one dense layer which is\n","the output layer after the pooled feature maps are flattened. Maybe introducing \n","a dense layer before the output layer with some activation function, probably relu,\n","will allow us to get the last extraction of learning the 1 dimensional data before \n","it hits the softmax output layer. Also, it may help to work with different neurons \n","in that dense layer as well.\n","\n","Finally, I want to touch on different parameters for the compilation of our model\n","settings as a well as different fit settings. Perhaps changing our optimizer will\n","help us towards our learning goal (sgd, rmsprop, etc.) or changing the learning rate \n","itself to help us towards our learning goal. For fitting, I think working with \n","different batch sizes will help when it comes to optimizing training time as well\n","as learning. \"\"\"\n","\n","pass"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"V_bAj22SFwda"},"outputs":[{"name":"stdout","output_type":"stream","text":["------------------------------------------------------------\n","Dense layers: 0, Convolutional layers: 1\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.208613395690918 accuracy: 58.66000056266785%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 0, Convolutional layers: 2\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.1549197435379028 accuracy: 61.19999885559082%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 0, Convolutional layers: 3\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.143257737159729 accuracy: 60.75999736785889%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 1, Convolutional layers: 1\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.3393447399139404 accuracy: 52.53000259399414%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 1, Convolutional layers: 2\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.1512749195098877 accuracy: 59.75000262260437%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 1, Convolutional layers: 3\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.1395007371902466 accuracy: 59.769999980926514%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 2, Convolutional layers: 1\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.3456608057022095 accuracy: 52.85999774932861%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 2, Convolutional layers: 2\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.1543594598770142 accuracy: 61.479997634887695%\n","------------------------------------------------------------\n","------------------------------------------------------------\n","Dense layers: 2, Convolutional layers: 3\n","------------------------------------------------------------\n","Peformance\n","------------------------------------------------------------\n","loss: 1.149789571762085 accuracy: 60.17000079154968%\n","------------------------------------------------------------\n"]}],"source":["\n","\"\"\"\n","\n","To automate the process of fine tuning this last model. I will create a function \n","that creates different models with different settings. I will then evaluate the model\n","using the testing dataset we created.\n","\n","The first parameters I want to play around with are the amount of convolutional layers \n","and adding (a) dense layer(s) after the max pooled features are flattened.\n","I'm hoping and hypothesizing that adding dense layers after the feature maps are flattened\n","will eak out some more learnable parameters. To this, I will also add a dropout layer for each \n","dense layer added before the output layer. In short dropout layers drop inputs at some rate which is good\n","because it allows the model to learn different parameters rather than just one entire training point. This is\n","good to prevent over-fitting of the data. I think the dropout layers may yield better testing \n","validation accuracy so I want all inputs dropped at some rate (20% may change) as they pass through the\n","dense layer(s) before it hits the output layer.\n","\n","I am going to test a maximum of 3 convolutional layers and a maximum of 2 dense layers that proceed. \n","The very base model will have zero dense + dropout layers and a single convlutional + max pooling layer.\n","The featuremaps created between layers will remain 32 and the neurons will also remain 32. the batch\n","size as it learns will be 64 which I find to be a good sweet spot for learning our parameters but this\n","can change as we progress\n","\"\"\"\n","\n","Dense_layers_added = [0, 1, 2] # up to 2 or 0 total dense + dropout layers\n","Conv2D_layers_added = [0, 1, 2] # up to 3 or 1 total convolutional + max pooling layers\n","\n","def create_models():\n","\n","    models = []\n","\n","    # trying every possible dense layer added last\n","\n","    for Dense_layer in Dense_layers_added:\n","\n","        # trying every additional convolutional layers added next\n","\n","        for Conv2D_layer in Conv2D_layers_added:\n","\n","            # trying every possible neuron (feature maps) amount first\n","\n","                \n","                # model settings and peformance\n","                print(f\"{'-' * 60}\\nDense layers: {Dense_layer}, Convolutional layers: {Conv2D_layer + 1}\\n{'-'* 60}\")\n","\n","                model = Sequential() # initializing model\n","\n","                model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 1), activation=\"relu\")) # adding base conv2D (remains constant for inputs)\n","                model.add(MaxPooling2D(pool_size=(2, 2))) # add MaxPooling\n","\n","                # adding i amount of additional convoluted layers with the same setup (different amount of feature maps)\n","                for i in range(Conv2D_layer):\n","                    model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n","                    model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","                model.add(Flatten()) # flattening pooled featuremaps\n","\n","                # adding j amount of dense & dropout layers with the same setup (different neurons)\n","                for j in range(Dense_layer):\n","                    model.add(Dense(32, activation=\"relu\"))\n","                    model.add(Dropout(0.2))\n","                \n","                model.add(Dense(10, activation=\"softmax\")) # adding our output layer with softmax\n","\n","                model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) # compiling with constant settings\n","                \n","                model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=0)\n","                loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n","                print(f\"Peformance\\n{'-'* 60}\\nloss: {loss} accuracy: {accuracy * 100}%\\n{'-'* 60}\")\n","\n","                # adding model to all our models to evaluate them (above the 60% accuracy threshold)\n","                if accuracy >= 0.6:\n","                    models.append((model, Conv2D_layer, Dense_layer, loss, accuracy))\n","\n","    return models\n","\n","\n","testing_models = create_models()\n","\n"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"Qj5kMMgpFwda"},"outputs":[{"name":"stdout","output_type":"stream","text":["[(<keras.engine.sequential.Sequential object at 0x7f801a4737f0>, 2, 2, 1.149789571762085, 0.6017000079154968), (<keras.engine.sequential.Sequential object at 0x7f8119ab9730>, 2, 0, 1.143257737159729, 0.6075999736785889), (<keras.engine.sequential.Sequential object at 0x7f80b009a5b0>, 1, 0, 1.1549197435379028, 0.6119999885559082), (<keras.engine.sequential.Sequential object at 0x7f80dc2a9100>, 1, 2, 1.1543594598770142, 0.614799976348877)]\n"]}],"source":["\"\"\"\n","Now that we have the models created, we can sort the models in ascending order\n","based on the validation accuracy and then pluck that model as the best model\n","over our test run.\n","\n","After previewing some of the settings, I'm surprised to see some models get through and some models not get through. \n","After further consideration a think a good model has 2-3 conv2D + MaxPooling layers and 0-1 Dense + Dropout layers. \n","Some models may have converged better than others on a given run so I won't be too picky and choose the absolute best\n","validation accuracy in this list\n","\"\"\"\n","\n","testing_models = sorted(testing_models, key=lambda x: x[-1])\n","\n","print(testing_models) # sorted overview in ascending\n","\n","\n"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"aQTRhpL6Fwdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model presets: 3 Conv2D + MaxPooling2D layers & 0 Dense + Dropout layer\n","-----------------------------------------------------------------\n","feature maps & neurons: 32, padding settings: valid\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","Validation loss: 1.0990355014801025, Validation accuracy: 61.6599977016449%\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","feature maps & neurons: 32, padding settings: same\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","Validation loss: 0.9683698415756226, Validation accuracy: 66.64999723434448%\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","feature maps & neurons: 64, padding settings: valid\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","Validation loss: 1.0446521043777466, Validation accuracy: 64.20999765396118%\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","feature maps & neurons: 64, padding settings: same\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","Validation loss: 0.8774350881576538, Validation accuracy: 70.46999931335449%\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","feature maps & neurons: 128, padding settings: valid\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","Validation loss: 0.9629859328269958, Validation accuracy: 67.8600013256073%\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","feature maps & neurons: 128, padding settings: same\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","Validation loss: 0.9162403345108032, Validation accuracy: 71.1899995803833%\n","-----------------------------------------------------------------\n"]}],"source":["\"\"\"\n","Now that we have a good idea of what layers to have to the model, let's\n","consider some other model settings that we can change. One thing I think will help\n","change and benefit our model is the amount of neurons in the dense layer as well\n","as how many feature maps are created as they pass through the convolutional and max pooling\n","layers. \n","\n","While testing this, I will also test the padding as valid vs padding as same. The \n","padding will affect the output of the filter when its applied to the image. It will either \n","keep image the same or downscale the image for the settings for the layers I have created.\n","Perhaps keeping the output from the convolutional layer through the filter the same\n","before features are pooled is better than decreasing the size from the edges and corners\n","of the image not completely hitting the filter. \n","\"\"\"\n","\n","neurons = [32, 64, 128] # 32 - 128 neurons\n","padding_settings = [\"valid\", \"same\"] # kernal operations with padding or without padding\n","\n","def create_models_2(conv_layers, dense_layers):\n","\n","    # model settings shown\n","    print(f\"Model presets: {conv_layers} Conv2D + MaxPooling2D layers & {dense_layers} Dense + Dropout layer\")\n","\n","    models = []\n","\n","    for neuron in neurons:\n","        \n","        # trying pad settings\n","        for padding in padding_settings:\n","            \n","            # displaying settings\n","            print(f\"{'-'* 65}\\nfeature maps & neurons: {neuron}, padding settings: {padding}\\n{'-'* 65}\")\n","\n","            model = Sequential() # init model\n","\n","            # adding conv2D + pooling layer input layer\n","            model.add(Conv2D(neuron, (3, 3), input_shape=(32, 32, 1), activation=\"relu\", padding=padding)) # changing pad settings\n","            model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","            # any additional conv2D + pooling layers added\n","            for i in range(conv_layers - 1):\n","                model.add(Conv2D(neuron, (3, 3), activation=\"relu\", padding=padding)) # changing pad settings\n","                model.add(MaxPooling2D(pool_size=(2, 2)))\n","            \n","            model.add(Flatten()) # flattening feature maps into 1D\n","\n","            # adding dense and drop layers\n","            for j in range(dense_layers):\n","                model.add(Dense(neuron, activation=\"relu\"))\n","                model.add(Dropout(0.2)) # dropping inputs at a rate of 20%\n","                \n","            model.add(Dense(10, activation=\"softmax\")) # output layer\n","\n","            # same compilation settings\n","            model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","            # same fit settings\n","            model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=0)\n","            loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n","\n","            # model peformance\n","            print(f\"{'-' * 65}\\nValidation loss: {loss}, Validation accuracy: {accuracy*100}%\\n{'-' * 65}\")\n","\n","            # only adding models that surpass 65% validation accuracy (best of the best)\n","            if accuracy >= 0.65:\n","                models.append((model, neuron, loss, accuracy))\n","\n","    return models\n","\n","testing_models_2 = create_models_2(3, 0) # the best layer setup up in my opinion\n","\n"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"qLJCNhThFwdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------\n","models that meet threshold: 4\n","-----------------------------------------------------------------\n","models: [(<keras.engine.sequential.Sequential object at 0x7f7fec556430>, 32, 0.9683698415756226, 0.6664999723434448), (<keras.engine.sequential.Sequential object at 0x7f809cb65df0>, 128, 0.9629859328269958, 0.678600013256073), (<keras.engine.sequential.Sequential object at 0x7f815cdfdbe0>, 64, 0.8774350881576538, 0.7046999931335449), (<keras.engine.sequential.Sequential object at 0x7f807c46c2b0>, 128, 0.9162403345108032, 0.711899995803833)]\n","-----------------------------------------------------------------\n"]}],"source":["\"\"\"\n","Now that we have the best models above 65% validation accuracy we can pluck the best \n","one or choose one we feel is the best given the ones that meet the threshold. The reason\n","I don't want to be too direct (picking the one with the best accuracy) when it comes picking\n","one is because the fact that one model may have converged better on a given instance but in \n","another instance may not have converged as well. So I really want to analyze the top 3 models\n","(if 3 models pass) and see some commonality between them to build one last model for parameter testing\n","\n","It appears having the padding as same perist throughout most of the models where the validation accuracy is above \n","65% with an exception of a valid padding model, although this model plenty of features maps created for the convolutional\n","layers.\n","\n","With this in mind I will start creating the last model with some minor changes to see if we get some different results\n","\"\"\"\n","\n","print(f\"{'-'* 65}\\nmodels that meet threshold: {len(testing_models_2)}\\n{'-'* 65}\\nmodels: {sorted(testing_models_2, key= lambda x: x[-1])}\\n{'-'* 65}\")\n"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Model presets: 3 Conv2D + MaxPooling2D layers & 0 Dense + Dropout layer\n","----------------------------------------\n","----------------------------------------\n","Batch size: 32\n","----------------------------------------\n","Epoch 1/10\n","1563/1563 [==============================] - 111s 71ms/step - loss: 1.4703 - accuracy: 0.4789\n","Epoch 2/10\n","1563/1563 [==============================] - 118s 75ms/step - loss: 1.0365 - accuracy: 0.6452\n","Epoch 3/10\n","1563/1563 [==============================] - 111s 71ms/step - loss: 0.8851 - accuracy: 0.6956\n","Epoch 4/10\n","1563/1563 [==============================] - 112s 72ms/step - loss: 0.7840 - accuracy: 0.7286\n","Epoch 5/10\n","1563/1563 [==============================] - 113s 72ms/step - loss: 0.7080 - accuracy: 0.7567\n","Epoch 6/10\n","1563/1563 [==============================] - 113s 73ms/step - loss: 0.6402 - accuracy: 0.7789\n","Epoch 7/10\n","1563/1563 [==============================] - 113s 72ms/step - loss: 0.5842 - accuracy: 0.7973\n","Epoch 8/10\n","1563/1563 [==============================] - 109s 70ms/step - loss: 0.5365 - accuracy: 0.8145\n","Epoch 9/10\n","1563/1563 [==============================] - 107s 69ms/step - loss: 0.4918 - accuracy: 0.8304\n","Epoch 10/10\n","1563/1563 [==============================] - 113s 72ms/step - loss: 0.4489 - accuracy: 0.8425\n","313/313 [==============================] - 6s 18ms/step - loss: 0.9638 - accuracy: 0.7002\n","-----------------------------------------------------------------\n","loss: 0.9638119339942932, -----------------------------------------------------------------\n","acc: 70.02000212669373%\n","-----------------------------------------------------------------\n","----------------------------------------\n","Batch size: 64\n","----------------------------------------\n","Epoch 1/10\n","782/782 [==============================] - 102s 131ms/step - loss: 1.5523 - accuracy: 0.4469\n","Epoch 2/10\n","782/782 [==============================] - 101s 129ms/step - loss: 1.0991 - accuracy: 0.6211\n","Epoch 3/10\n","782/782 [==============================] - 102s 131ms/step - loss: 0.9406 - accuracy: 0.6770\n","Epoch 4/10\n","782/782 [==============================] - 102s 131ms/step - loss: 0.8359 - accuracy: 0.7141\n","Epoch 5/10\n","782/782 [==============================] - 101s 129ms/step - loss: 0.7609 - accuracy: 0.7391\n","Epoch 6/10\n","782/782 [==============================] - 97s 124ms/step - loss: 0.6943 - accuracy: 0.7623\n","Epoch 7/10\n","782/782 [==============================] - 97s 125ms/step - loss: 0.6285 - accuracy: 0.7824\n","Epoch 8/10\n","782/782 [==============================] - 98s 125ms/step - loss: 0.5788 - accuracy: 0.8012\n","Epoch 9/10\n","782/782 [==============================] - 99s 127ms/step - loss: 0.5308 - accuracy: 0.8173\n","Epoch 10/10\n","782/782 [==============================] - 94s 121ms/step - loss: 0.4873 - accuracy: 0.8315\n","313/313 [==============================] - 6s 19ms/step - loss: 0.8743 - accuracy: 0.7207\n","-----------------------------------------------------------------\n","loss: 0.8743001222610474, -----------------------------------------------------------------\n","acc: 72.07000255584717%\n","-----------------------------------------------------------------\n","----------------------------------------\n","Batch size: 128\n","----------------------------------------\n","Epoch 1/10\n","391/391 [==============================] - 96s 242ms/step - loss: 1.6707 - accuracy: 0.4040\n","Epoch 2/10\n","391/391 [==============================] - 93s 238ms/step - loss: 1.2186 - accuracy: 0.5787\n","Epoch 3/10\n","391/391 [==============================] - 89s 228ms/step - loss: 1.0403 - accuracy: 0.6435\n","Epoch 4/10\n","391/391 [==============================] - 94s 241ms/step - loss: 0.9378 - accuracy: 0.6802\n","Epoch 5/10\n","391/391 [==============================] - 95s 243ms/step - loss: 0.8577 - accuracy: 0.7059\n","Epoch 6/10\n","391/391 [==============================] - 105s 269ms/step - loss: 0.7946 - accuracy: 0.7276\n","Epoch 7/10\n","391/391 [==============================] - 96s 246ms/step - loss: 0.7441 - accuracy: 0.7470\n","Epoch 8/10\n","391/391 [==============================] - 134s 343ms/step - loss: 0.7003 - accuracy: 0.7619\n","Epoch 9/10\n","391/391 [==============================] - 94s 239ms/step - loss: 0.6481 - accuracy: 0.7788\n","Epoch 10/10\n","391/391 [==============================] - 94s 241ms/step - loss: 0.6073 - accuracy: 0.7937\n","313/313 [==============================] - 6s 18ms/step - loss: 0.8375 - accuracy: 0.7226\n","-----------------------------------------------------------------\n","loss: 0.8375136852264404, -----------------------------------------------------------------\n","acc: 72.2599983215332%\n","-----------------------------------------------------------------\n"]}],"source":["\"\"\"\n","I believe the adam optimizer is the best optimizer for this problem after doing further research.\n","With the validation results we have, changing the learning rate could possibly hurt the models \n","peformance so I will maintain the default learning rate adam has in the built in class.\n","\n","The last metric I will tune will be the batch size which has an impact on how the model learns.\n","The batch size is essentially how much data our model sees before it updates the weights and biases.\n","I will try a variation of 3 batch sizes 32, 64, and 128 to see which batch size gives our model the best\n","peformance. \n","\"\"\"\n","\n","batch_sizes = [32, 64, 128]\n","print(f\"{'-' * 40}\\nModel presets: 3 Conv2D + MaxPooling2D layers & 0 Dense + Dropout layer\\n{'-' * 40}\")\n","stats = []\n","# trying all batch sizes\n","for batch_size in batch_sizes:\n","\n","    print(f\"{'-' * 40}\\nBatch size: {batch_size}\\n{'-' * 40}\")\n","\n","    \n","    model = Sequential() # model init\n","    model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 1), activation=\"relu\", padding=\"same\")) # input conv2D layer\n","    model.add(MaxPooling2D(pool_size=(2, 2))) # pooling features\n","\n","    # adding last 2 Conv2D + MaxPooling2D layers\n","    for i in range(2):\n","        model.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","    model.add(Flatten()) # flatten featuremaps\n","    model.add(Dense(10, activation=\"softmax\")) # output layer\n","\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) # same compilation settings\n","    model.fit(x_train, y_train, batch_size=batch_size, epochs=10, verbose=1) # same fit settings but trying batches\n","    loss, acc = model.evaluate(x_test, y_test)\n","    print(f\"{'-' * 65}\\nloss: {loss}, acc: {acc * 100}%\\n{'-' * 65}\")\n","\n","    # allowing models only past 70% threshold\n","    if acc >= 0.7:\n","        stats.append((batch_size, loss, acc))\n","\n"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------\n","Batch sizes based off validation accuracy sorted: [(32, 0.9638119339942932, 0.7002000212669373), (64, 0.8743001222610474, 0.7207000255584717), (128, 0.8375136852264404, 0.722599983215332)]\n","-----------------------------------------------------------------\n"]}],"source":["\"\"\"\n","Looking into it the larger batch sizes of 64 and 128 yield the best testing results.\n","Since the results are so close I am going to choose the smaller batch size because I\n","want the model to generalize better because we will present training data to it and\n","making predictions with it\n","\"\"\"\n","\n","print(f\"{'-' * 65}\\nBatch sizes based off validation accuracy sorted: {sorted(stats, key=lambda x: x[-1])}\\n{'-' * 65}\")"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_68\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_145 (Conv2D)         (None, 32, 32, 128)       1280      \n","                                                                 \n"," max_pooling2d_144 (MaxPooli  (None, 16, 16, 128)      0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_146 (Conv2D)         (None, 16, 16, 128)       147584    \n","                                                                 \n"," max_pooling2d_145 (MaxPooli  (None, 8, 8, 128)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_147 (Conv2D)         (None, 8, 8, 128)         147584    \n","                                                                 \n"," max_pooling2d_146 (MaxPooli  (None, 4, 4, 128)        0         \n"," ng2D)                                                           \n","                                                                 \n"," flatten_67 (Flatten)        (None, 2048)              0         \n","                                                                 \n"," dense_104 (Dense)           (None, 10)                20490     \n","                                                                 \n","=================================================================\n","Total params: 316,938\n","Trainable params: 316,938\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model_2 = Sequential() # model init\n","model_2.add(Conv2D(128, (3, 3), input_shape=(32, 32, 1), activation=\"relu\", padding=\"same\")) # input conv2D layer\n","model_2.add(MaxPooling2D(pool_size=(2, 2))) # pooling features\n","\n","# adding last 2 Conv2D + MaxPooling2D layers\n","for i in range(2):\n","    model_2.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n","    model_2.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model_2.add(Flatten()) # flatten feature maps\n","model_2.add(Dense(10, activation=\"softmax\")) # output layer\n","\n","model_2.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) # same compilation settings\n","model_2.summary()\n"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","782/782 [==============================] - 246s 313ms/step - loss: 1.5592 - accuracy: 0.4471\n","Epoch 2/10\n","782/782 [==============================] - 254s 325ms/step - loss: 1.1164 - accuracy: 0.6156\n","Epoch 3/10\n","782/782 [==============================] - 227s 290ms/step - loss: 0.9439 - accuracy: 0.6742\n","Epoch 4/10\n","782/782 [==============================] - 205s 263ms/step - loss: 0.8374 - accuracy: 0.7134\n","Epoch 5/10\n","782/782 [==============================] - 208s 266ms/step - loss: 0.7562 - accuracy: 0.7428\n","Epoch 6/10\n","782/782 [==============================] - 217s 277ms/step - loss: 0.6853 - accuracy: 0.7663\n","Epoch 7/10\n","782/782 [==============================] - 121s 154ms/step - loss: 0.6279 - accuracy: 0.7832\n","Epoch 8/10\n","782/782 [==============================] - 124s 159ms/step - loss: 0.5705 - accuracy: 0.8047\n","Epoch 9/10\n","782/782 [==============================] - 112s 144ms/step - loss: 0.5243 - accuracy: 0.8188\n","Epoch 10/10\n","782/782 [==============================] - 121s 155ms/step - loss: 0.4799 - accuracy: 0.8344\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f80bc3ac970>"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["model_2.fit(x_train, y_train, batch_size=64, epochs=10)"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["7450\n","313/313 [==============================] - 0s 1ms/step - loss: 1.7805 - accuracy: 0.3644\n","313/313 [==============================] - 1s 3ms/step - loss: 1.0903 - accuracy: 0.6269\n","313/313 [==============================] - 6s 20ms/step - loss: 0.8899 - accuracy: 0.7157\n"]}],"source":["\"\"\"\n","After plenty of training, evaluating and tuning we have found a model that peforms relatively will\n","on our training data. Here is an overview and some predictions on the data in the model\n","\"\"\"\n","\n","i = np.random.randint(len(x_test)) # random index\n","print(i)\n","\n","# evaluation data\n","loss_b, acc_b = baseline.evaluate(x_test, y_test)\n","pred_b, true_b = np.argmax(baseline.predict(x_test)[i]), y_test[i][0]\n","loss_1, acc_1 = model_1.evaluate(x_test, y_test)\n","pred_1, true_1 = np.argmax(model_1.predict(x_test)[i]), y_test[i][0]\n","loss_2, acc_2 = model_2.evaluate(x_test, y_test)\n","pred_2, true_2 = np.argmax(model_2.predict(x_test)[i]), y_test[i][0]\n","\n"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------------------------------------------\n","Model Peformances\n","----------------------------------------------------------------------------------------------------\n","Baseline Peformance\n","----------------------------------------------------------------------------------------------------\n","Loss: 1.7804551124572754 Accuracy: 36.43999993801117%\n","Prediction: 1 True: 1\n","----------------------------------------------------------------------------------------------------\n","Model 1 Peformance\n","----------------------------------------------------------------------------------------------------\n","Loss: 1.0902934074401855 Accuracy: 62.69000172615051%\n","Prediction: 1 True: 1\n","----------------------------------------------------------------------------------------------------\n","Model 2 Peformance\n","----------------------------------------------------------------------------------------------------\n","Loss: 0.8899046182632446 Accuracy: 71.56999707221985%\n","Prediction: 1 True: 1\n"]}],"source":["print(f\"{'-' * 100}\\nModel Peformances\")\n","print(f\"{'-' * 100}\\nBaseline Peformance\\n{'-' * 100}\\nLoss: {loss_b} Accuracy: {acc_b * 100}%\\nPrediction: {pred_b} True: {true_b}\")\n","print(f\"{'-' * 100}\\nModel 1 Peformance\\n{'-' * 100}\\nLoss: {loss_1} Accuracy: {acc_1 * 100}%\\nPrediction: {pred_1} True: {true_1}\")\n","print(f\"{'-' * 100}\\nModel 2 Peformance\\n{'-' * 100}\\nLoss: {loss_2} Accuracy: {acc_2 * 100}%\\nPrediction: {pred_2} True: {true_2}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Main Model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Main Model\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","\n","\n","x_train, x_test = np.array(x_train), np.array(x_test)\n","y_train, y_test = np.array(y_train), np.array(y_test)\n","\n","X = np.concatenate((x_train, x_test))\n","y = np.concatenate((y_train, y_test))"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def process(images):\n","    data = [] # new array tp hold gray scale images\n","    # images = images / 255.0 # normalizing every image between 0 and 1\n","    gamma = 1.04 # color gamma correction factor (smoother color transitions)\n","    r_cst, g_cst, b_cst = 0.2126, 0.7152, .0722 # constants for red, green, and blue gray scale conversion factor\n","    for img in images:\n","        r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2] # each rgb value for the image\n","        data.append((r_cst * r ** gamma) + (g_cst * g ** gamma) + (b_cst * b ** gamma))\n","    return np.array(data)\n","\n","X = process(X)\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["X = X / 255 # norm"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_9 (Conv2D)           (None, 32, 32, 128)       1280      \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 16, 16, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 16, 16, 128)       147584    \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 8, 8, 128)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 8, 8, 128)         147584    \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 4, 4, 128)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_3 (Flatten)         (None, 2048)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                20490     \n","                                                                 \n","=================================================================\n","Total params: 316,938\n","Trainable params: 316,938\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = Sequential() # model init\n","\n","model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 1), activation=\"relu\", padding=\"same\")) # input conv2D layer\n","model.add(MaxPooling2D(pool_size=(2, 2))) # pooling features\n","\n","# adding last 2 Conv2D + MaxPooling2D layers\n","for i in range(2):\n","    model.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Flatten()) # flatten feature maps\n","model.add(Dense(10, activation=\"softmax\")) # output layer\n","\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) # same compilation settings"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","938/938 [==============================] - 135s 143ms/step - loss: 1.5381 - accuracy: 0.4569\n","Epoch 2/10\n","938/938 [==============================] - 147s 156ms/step - loss: 1.0853 - accuracy: 0.6274\n","Epoch 3/10\n","938/938 [==============================] - 146s 155ms/step - loss: 0.9329 - accuracy: 0.6829\n","Epoch 4/10\n","938/938 [==============================] - 146s 155ms/step - loss: 0.8317 - accuracy: 0.7156\n","Epoch 5/10\n","938/938 [==============================] - 147s 156ms/step - loss: 0.7562 - accuracy: 0.7408\n","Epoch 6/10\n","938/938 [==============================] - 151s 161ms/step - loss: 0.6947 - accuracy: 0.7627\n","Epoch 7/10\n","938/938 [==============================] - 150s 160ms/step - loss: 0.6363 - accuracy: 0.7818\n","Epoch 8/10\n","938/938 [==============================] - 151s 160ms/step - loss: 0.5884 - accuracy: 0.7983\n","Epoch 9/10\n","938/938 [==============================] - 152s 162ms/step - loss: 0.5475 - accuracy: 0.8112\n","Epoch 10/10\n","938/938 [==============================] - 151s 161ms/step - loss: 0.5158 - accuracy: 0.8216\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fefd753ba60>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X, y, batch_size=64, epochs=10)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["import os\n","from preprocess import path\n","model.save(f\"{path}/model/model.h5\")"]}],"metadata":{"colab":{"name":"project_2.ipynb","provenance":[]},"interpreter":{"hash":"63847a374abcb3b9a1a98c0eeba015d87a63f2412b9d7a6f458ed5afc164c9c8"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
